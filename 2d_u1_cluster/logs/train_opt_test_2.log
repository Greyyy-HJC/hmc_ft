 
>>> PBS_NODEFILE content:
sophia-gpu-10.lab.alcf.anl.gov
1n*1t
Wed Jul  2 20:02:09 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:47:00.0 Off |                    0 |
| N/A   21C    P0             53W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          Off |   00000000:4E:00.0 Off |                    0 |
| N/A   21C    P0             53W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Thu_Mar_28_02:18:24_PDT_2024
Cuda compilation tools, release 12.4, V12.4.131
Build cuda_12.4.r12.4/compiler.34097967_0
Start time: 2025-07-02 20:02:10
Python 3.9.18
Python path: /lus/eagle/projects/fthmc/software/ml/bin/python
You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

============================================================
>>> CUDA device count: 2
PyTorch version: 2.5.0+cu124
CUDA available: True
>>> Arguments:
Lattice size: 64x64
Minimum beta: 2.0
Maximum beta: 2.0
Beta gap: 0.5
Number of epochs: 16
Batch size: 64
Number of subsets: 8
Number of workers: 0
Random seed: 2008
Identity initialization: True
Check Jacobian: False
Continue training: False
============================================================
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
Trying to use torch.compile for optimized computation...
Successfully initialized torch.compile
>>> Training from scratch
Loaded data shape: torch.Size([4096, 2, 64, 64])
Training data shape: torch.Size([3276, 2, 64, 64])
Testing data shape: torch.Size([820, 2, 64, 64])

>>> Training the model at beta =  Training epochs:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 1/16:   0%|          | 0/26 [00:00<?, ?it/s][A2.0

>>> Training the model at beta = 2.0

Training epochs:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 1/16:   0%|          | 0/26 [00:00<?, ?it/s][A

Epoch 1/16:   4%|â–         | 1/26 [00:16<06:43, 16.16s/it][AEpoch 1/16:   4%|â–         | 1/26 [00:16<06:43, 16.14s/it][A

Epoch 1/16:   8%|â–Š         | 2/26 [00:17<03:00,  7.54s/it][AEpoch 1/16:   8%|â–Š         | 2/26 [00:17<03:00,  7.53s/it][A

Epoch 1/16:  12%|â–ˆâ–        | 3/26 [00:19<01:49,  4.77s/it][AEpoch 1/16:  12%|â–ˆâ–        | 3/26 [00:19<01:49,  4.77s/it][A
Epoch 1/16:  15%|â–ˆâ–Œ        | 4/26 [00:20<01:16,  3.47s/it][A
Epoch 1/16:  15%|â–ˆâ–Œ        | 4/26 [00:20<01:16,  3.47s/it][A

Epoch 1/16:  19%|â–ˆâ–‰        | 5/26 [00:22<00:57,  2.75s/it][AEpoch 1/16:  19%|â–ˆâ–‰        | 5/26 [00:22<00:57,  2.75s/it][A
Epoch 1/16:  23%|â–ˆâ–ˆâ–Ž       | 6/26 [00:23<00:46,  2.31s/it][A
Epoch 1/16:  23%|â–ˆâ–ˆâ–Ž       | 6/26 [00:23<00:46,  2.31s/it][A

Epoch 1/16:  27%|â–ˆâ–ˆâ–‹       | 7/26 [00:25<00:40,  2.14s/it]Epoch 1/16:  27%|â–ˆâ–ˆâ–‹       | 7/26 [00:25<00:40,  2.14s/it][A[A
Epoch 1/16:  31%|â–ˆâ–ˆâ–ˆ       | 8/26 [00:26<00:35,  1.97s/it][A
Epoch 1/16:  31%|â–ˆâ–ˆâ–ˆ       | 8/26 [00:26<00:35,  1.97s/it][A
Epoch 1/16:  35%|â–ˆâ–ˆâ–ˆâ–      | 9/26 [00:28<00:31,  1.84s/it][A
Epoch 1/16:  35%|â–ˆâ–ˆâ–ˆâ–      | 9/26 [00:28<00:31,  1.84s/it][A
Epoch 1/16:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 10/26 [00:30<00:28,  1.76s/it][A
Epoch 1/16:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 10/26 [00:30<00:28,  1.76s/it][A
Epoch 1/16:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/26 [00:31<00:25,  1.70s/it][A
Epoch 1/16:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/26 [00:31<00:25,  1.70s/it][A
Epoch 1/16:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 12/26 [00:33<00:23,  1.66s/it][A
Epoch 1/16:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 12/26 [00:33<00:23,  1.66s/it][A
Epoch 1/16:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 13/26 [00:34<00:21,  1.66s/it][A
Epoch 1/16:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 13/26 [00:34<00:21,  1.66s/it][A
Epoch 1/16:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/26 [00:36<00:20,  1.67s/it][A
Epoch 1/16:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/26 [00:36<00:20,  1.67s/it][A
Epoch 1/16:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 15/26 [00:38<00:18,  1.67s/it][A
Epoch 1/16:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 15/26 [00:38<00:18,  1.67s/it][A
Epoch 1/16:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/26 [00:39<00:16,  1.70s/it][A
Epoch 1/16:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/26 [00:40<00:16,  1.70s/it][A
Epoch 1/16:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 17/26 [00:41<00:15,  1.74s/it][A
Epoch 1/16:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 17/26 [00:41<00:15,  1.74s/it][A
Epoch 1/16:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 18/26 [00:43<00:14,  1.81s/it][A
Epoch 1/16:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 18/26 [00:43<00:14,  1.81s/it][A
Epoch 1/16:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 19/26 [00:45<00:12,  1.83s/it][A
Epoch 1/16:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 19/26 [00:45<00:12,  1.83s/it][A
Epoch 1/16:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 20/26 [00:47<00:11,  1.85s/it][A
Epoch 1/16:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 20/26 [00:47<00:11,  1.85s/it][A
Epoch 1/16:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 21/26 [00:49<00:09,  1.85s/it][A
Epoch 1/16:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 21/26 [00:49<00:09,  1.85s/it][A
Epoch 1/16:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/26 [00:51<00:07,  1.83s/it][A
Epoch 1/16:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/26 [00:51<00:07,  1.83s/it][A
Epoch 1/16:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 23/26 [00:52<00:05,  1.82s/it][A
Epoch 1/16:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 23/26 [00:53<00:05,  1.82s/it][A
Epoch 1/16:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 24/26 [00:54<00:03,  1.81s/it][A
Epoch 1/16:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 24/26 [00:54<00:03,  1.81s/it][A
Epoch 1/16:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 25/26 [00:56<00:01,  1.80s/it][A
Epoch 1/16:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 25/26 [00:56<00:01,  1.80s/it][A
Epoch 1/16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:59<00:00,  2.20s/it][AEpoch 1/16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:59<00:00,  2.30s/it]

Epoch 1/16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:59<00:00,  2.20s/it][AEpoch 1/16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:59<00:00,  2.30s/it]

Evaluating:   0%|          | 0/7 [00:00<?, ?it/s][A
Evaluating:   0%|          | 0/7 [00:00<?, ?it/s][A

Evaluating:  14%|â–ˆâ–        | 1/7 [00:00<00:03,  1.97it/s][AEvaluating:  14%|â–ˆâ–        | 1/7 [00:00<00:03,  1.97it/s][A
Evaluating:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:02,  1.93it/s][A
Evaluating:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:02,  1.92it/s][A
Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:01<00:02,  1.94it/s][A
Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:01<00:02,  1.94it/s][A
Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:02<00:01,  1.96it/s][A
Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:02<00:01,  1.95it/s][A

Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:02<00:01,  1.97it/s][AEvaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:02<00:01,  1.98it/s][A
Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:03<00:00,  1.98it/s][A
Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:03<00:00,  1.95it/s][A
Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:03<00:00,  2.02it/s][AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:03<00:00,  1.99it/s]
Epoch 1/16 - Train Loss: 15.426671 - Test Loss: 12.171457

Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:03<00:00,  1.98it/s][AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:03<00:00,  1.96it/s]
Training epochs:   6%|â–‹         | 1/16 [01:03<15:52, 63.48s/it]Training epochs:   6%|â–‹         | 1/16 [01:03<15:51, 63.46s/it]
Epoch 2/16:   0%|          | 0/26 [00:00<?, ?it/s][A
Epoch 2/16:   0%|          | 0/26 [00:00<?, ?it/s][A
Epoch 2/16:   4%|â–         | 1/26 [00:01<00:43,  1.75s/it][A
Epoch 2/16:   4%|â–         | 1/26 [00:01<00:43,  1.75s/it][A
Epoch 2/16:   8%|â–Š         | 2/26 [00:03<00:42,  1.78s/it][A
Epoch 2/16:   8%|â–Š         | 2/26 [00:03<00:42,  1.78s/it][A
Epoch 2/16:  12%|â–ˆâ–        | 3/26 [00:05<00:41,  1.80s/it][A
Epoch 2/16:  12%|â–ˆâ–        | 3/26 [00:05<00:41,  1.80s/it][A
Epoch 2/16:  15%|â–ˆâ–Œ        | 4/26 [00:07<00:39,  1.81s/it][A
Epoch 2/16:  15%|â–ˆâ–Œ        | 4/26 [00:07<00:39,  1.81s/it][A
Epoch 2/16:  19%|â–ˆâ–‰        | 5/26 [00:09<00:38,  1.81s/it][A
Epoch 2/16:  19%|â–ˆâ–‰        | 5/26 [00:09<00:38,  1.81s/it][A
Epoch 2/16:  23%|â–ˆâ–ˆâ–Ž       | 6/26 [00:10<00:36,  1.82s/it][A
Epoch 2/16:  23%|â–ˆâ–ˆâ–Ž       | 6/26 [00:10<00:36,  1.82s/it][A
Epoch 2/16:  27%|â–ˆâ–ˆâ–‹       | 7/26 [00:12<00:34,  1.82s/it][A
Epoch 2/16:  27%|â–ˆâ–ˆâ–‹       | 7/26 [00:12<00:34,  1.82s/it][A
Epoch 2/16:  31%|â–ˆâ–ˆâ–ˆ       | 8/26 [00:14<00:32,  1.82s/it][A
Epoch 2/16:  31%|â–ˆâ–ˆâ–ˆ       | 8/26 [00:14<00:32,  1.83s/it][A
Epoch 2/16:  35%|â–ˆâ–ˆâ–ˆâ–      | 9/26 [00:16<00:30,  1.82s/it][A
Epoch 2/16:  35%|â–ˆâ–ˆâ–ˆâ–      | 9/26 [00:16<00:30,  1.82s/it][A
Epoch 2/16:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 10/26 [00:18<00:30,  1.88s/it][A
Epoch 2/16:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 10/26 [00:18<00:30,  1.88s/it][A
Epoch 2/16:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/26 [00:20<00:27,  1.85s/it][A
Epoch 2/16:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/26 [00:20<00:27,  1.86s/it][A
Epoch 2/16:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 12/26 [00:21<00:25,  1.83s/it][A
Epoch 2/16:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 12/26 [00:21<00:25,  1.83s/it][A
Epoch 2/16:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 13/26 [00:23<00:23,  1.81s/it][A
Epoch 2/16:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 13/26 [00:23<00:23,  1.81s/it][A
Epoch 2/16:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/26 [00:25<00:21,  1.80s/it][A
Epoch 2/16:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/26 [00:25<00:21,  1.80s/it][A
Epoch 2/16:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 15/26 [00:27<00:19,  1.79s/it][A
Epoch 2/16:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 15/26 [00:27<00:19,  1.80s/it][A
Epoch 2/16:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/26 [00:29<00:17,  1.80s/it][A
Epoch 2/16:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/26 [00:29<00:17,  1.80s/it][A
Epoch 2/16:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 17/26 [00:30<00:16,  1.80s/it][A
Epoch 2/16:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 17/26 [00:30<00:16,  1.80s/it][A
Epoch 2/16:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 18/26 [00:32<00:14,  1.80s/it][A
Epoch 2/16:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 18/26 [00:32<00:14,  1.80s/it][A
Epoch 2/16:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 19/26 [00:34<00:12,  1.80s/it][A
Epoch 2/16:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 19/26 [00:34<00:12,  1.80s/it][A
Epoch 2/16:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 20/26 [00:36<00:10,  1.80s/it][A
Epoch 2/16:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 20/26 [00:36<00:10,  1.80s/it][A
Epoch 2/16:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 21/26 [00:38<00:09,  1.85s/it][A
Epoch 2/16:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 21/26 [00:38<00:09,  1.85s/it][A
Epoch 2/16:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/26 [00:39<00:07,  1.83s/it][A
Epoch 2/16:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/26 [00:39<00:07,  1.83s/it][A
Epoch 2/16:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 23/26 [00:41<00:05,  1.82s/it][A
Epoch 2/16:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 23/26 [00:41<00:05,  1.82s/it][AW0702 20:05:09.542888 3701371 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGTERM death signal, shutting down workers
W0702 20:05:09.543682 3701371 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3702987 closing signal SIGTERM
W0702 20:05:09.544146 3701371 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3702988 closing signal SIGTERM
Traceback (most recent call last):
  File "/lus/eagle/projects/fthmc/software/ml/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 696, in run
    result = self._invoke_run(role)
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 855, in _invoke_run
    time.sleep(monitor_interval)
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3701371 got signal: 15
