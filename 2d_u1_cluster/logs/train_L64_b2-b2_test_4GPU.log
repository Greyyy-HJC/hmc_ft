 
>>> PBS_NODEFILE content:
sophia-gpu-02.lab.alcf.anl.gov
1n*1t
Sat Jun 28 02:37:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:47:00.0 Off |                    0 |
| N/A   20C    P0             51W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          Off |   00000000:4E:00.0 Off |                    0 |
| N/A   21C    P0             51W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          Off |   00000000:87:00.0 Off |                    0 |
| N/A   25C    P0             51W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-40GB          Off |   00000000:90:00.0 Off |                    0 |
| N/A   23C    P0             52W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Thu_Mar_28_02:18:24_PDT_2024
Cuda compilation tools, release 12.4, V12.4.131
Build cuda_12.4.r12.4/compiler.34097967_0
Start time: 2025-06-28 02:37:27
Python 3.9.18
Python path: /lus/eagle/projects/fthmc/software/ml/bin/python
Found 4 GPUs, starting DDP training...
[W628 02:37:33.006890764 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W628 02:37:33.059245349 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[rank3]:[W628 02:37:34.594088686 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[W628 02:37:34.629721647 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[rank2]:[W628 02:37:34.634566129 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[W628 02:37:34.636757503 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[rank0]:[W628 02:37:34.096417030 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W628 02:37:34.107903343 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:I0628 02:37:38.849043 3343214 torch/_dynamo/backends/distributed.py:82] [2/0] 
[rank0]:I0628 02:37:38.849043 3343214 torch/_dynamo/backends/distributed.py:82] [2/0] DDPOptimizer used bucket cap 26214400 and created 1 buckets. Enable debug logs for detailed bucket info.
[rank1]:I0628 02:37:38.849445 3343215 torch/_dynamo/backends/distributed.py:82] [2/0] 
[rank1]:I0628 02:37:38.849445 3343215 torch/_dynamo/backends/distributed.py:82] [2/0] DDPOptimizer used bucket cap 26214400 and created 1 buckets. Enable debug logs for detailed bucket info.
[rank3]:I0628 02:37:38.860712 3343217 torch/_dynamo/backends/distributed.py:82] [2/0] 
[rank3]:I0628 02:37:38.860712 3343217 torch/_dynamo/backends/distributed.py:82] [2/0] DDPOptimizer used bucket cap 26214400 and created 1 buckets. Enable debug logs for detailed bucket info.
[rank2]:I0628 02:37:38.942645 3343216 torch/_dynamo/backends/distributed.py:82] [2/0] 
[rank2]:I0628 02:37:38.942645 3343216 torch/_dynamo/backends/distributed.py:82] [2/0] DDPOptimizer used bucket cap 26214400 and created 1 buckets. Enable debug logs for detailed bucket info.
[rank0]:I0628 02:37:40.828772 3343214 torch/_dynamo/backends/distributed.py:82] [2/1] 
[rank0]:I0628 02:37:40.828772 3343214 torch/_dynamo/backends/distributed.py:82] [2/1] DDPOptimizer used bucket cap 26214400 and created 1 buckets. Enable debug logs for detailed bucket info.
[rank2]:I0628 02:37:40.836064 3343216 torch/_dynamo/backends/distributed.py:82] [2/1] 
[rank2]:I0628 02:37:40.836064 3343216 torch/_dynamo/backends/distributed.py:82] [2/1] DDPOptimizer used bucket cap 26214400 and created 1 buckets. Enable debug logs for detailed bucket info.
[rank1]:I0628 02:37:40.864983 3343215 torch/_dynamo/backends/distributed.py:82] [2/1] 
[rank1]:I0628 02:37:40.864983 3343215 torch/_dynamo/backends/distributed.py:82] [2/1] DDPOptimizer used bucket cap 26214400 and created 1 buckets. Enable debug logs for detailed bucket info.
[rank3]:I0628 02:37:40.902034 3343217 torch/_dynamo/backends/distributed.py:82] [2/1] 
[rank3]:I0628 02:37:40.902034 3343217 torch/_dynamo/backends/distributed.py:82] [2/1] DDPOptimizer used bucket cap 26214400 and created 1 buckets. Enable debug logs for detailed bucket info.
[rank2]:I0628 02:37:55.268523 3343216 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank0]:I0628 02:37:55.268529 3343214 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank3]:I0628 02:37:55.268561 3343217 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank1]:I0628 02:37:55.268560 3343215 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank0]:I0628 02:37:55.278699 3343214 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank1]:I0628 02:37:55.279071 3343215 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank2]:I0628 02:37:55.279597 3343216 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank3]:I0628 02:37:55.279608 3343217 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank0]:I0628 02:37:55.288102 3343214 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank1]:I0628 02:37:55.288192 3343215 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank2]:I0628 02:37:55.289844 3343216 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank3]:I0628 02:37:55.289856 3343217 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank0]:I0628 02:37:55.297715 3343214 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank1]:I0628 02:37:55.297962 3343215 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank3]:I0628 02:37:55.300259 3343217 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank2]:I0628 02:37:55.300254 3343216 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank0]:I0628 02:37:55.306710 3343214 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank1]:I0628 02:37:55.306975 3343215 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank3]:I0628 02:37:55.310138 3343217 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank2]:I0628 02:37:55.310204 3343216 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank0]:I0628 02:37:55.315711 3343214 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank1]:I0628 02:37:55.315969 3343215 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank3]:I0628 02:37:55.320034 3343217 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank2]:I0628 02:37:55.320099 3343216 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank0]:I0628 02:37:55.324365 3343214 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank1]:I0628 02:37:55.325357 3343215 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank3]:I0628 02:37:55.329175 3343217 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank2]:I0628 02:37:55.329506 3343216 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank0]:I0628 02:37:55.333635 3343214 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank1]:I0628 02:37:55.334568 3343215 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank3]:I0628 02:37:55.338805 3343217 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
[rank2]:I0628 02:37:55.338906 3343216 torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
Trying to use torch.compile for optimized computation...
Successfully initialized torch.compile
Trying to use torch.compile for optimized computation...
Successfully initialized torch.compile
sophia-gpu-02:3343216:3343216 [2] NCCL INFO cudaDriverVersion 12040
sophia-gpu-02:3343216:3343216 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker0
sophia-gpu-02:3343216:3343216 [2] NCCL INFO Bootstrap : Using enp226s0:10.230.2.190<0>
sophia-gpu-02:3343216:3343216 [2] NCCL INFO NCCL version 2.22.3+cuda12.4
sophia-gpu-02:3343216:3343755 [2] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal network plugin.
sophia-gpu-02:3343216:3343755 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker0
sophia-gpu-02:3343216:3343755 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/RoCE [6]mlx5_6:1/IB [7]mlx5_7:1/IB [8]mlx5_8:1/IB [9]mlx5_9:1/IB [10]mlx5_10:1/IB [11]mlx5_11:1/RoCE [RO]; OOB enp226s0:10.230.2.190<0>
sophia-gpu-02:3343216:3343755 [2] NCCL INFO Using network IB
sophia-gpu-02:3343216:3343755 [2] NCCL INFO ncclCommInitRank comm 0x561b08925010 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 87000 commId 0xca63a03d39dbc1c9 - Init START
sophia-gpu-02:3343216:3343755 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000,00000000
sophia-gpu-02:3343216:3343755 [2] NCCL INFO NVLS multicast support is not available on dev 2
sophia-gpu-02:3343216:3343755 [2] NCCL INFO comm 0x561b08925010 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
sophia-gpu-02:3343216:3343755 [2] NCCL INFO Trees [0] -1/-1/-1->2->3 [1] -1/-1/-1->2->3 [2] -1/-1/-1->2->3 [3] -1/-1/-1->2->3 [4] -1/-1/-1->2->3 [5] -1/-1/-1->2->3 [6] -1/-1/-1->2->3 [7] -1/-1/-1->2->3 [8] -1/-1/-1->2->3 [9] -1/-1/-1->2->3 [10] -1/-1/-1->2->3 [11] -1/-1/-1->2->3 [12] -1/-1/-1->2->3 [13] -1/-1/-1->2->3 [14] -1/-1/-1->2->3 [15] -1/-1/-1->2->3 [16] -1/-1/-1->2->3 [17] -1/-1/-1->2->3 [18] -1/-1/-1->2->3 [19] -1/-1/-1->2->3 [20] -1/-1/-1->2->3 [21] -1/-1/-1->2->3 [22] -1/-1/-1->2->3 [23] -1/-1/-1->2->3
sophia-gpu-02:3343216:3343755 [2] NCCL INFO P2P Chunksize set to 524288
sophia-gpu-02:3343216:3343755 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
sophia-gpu-02:3343216:3343755 [2] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
sophia-gpu-02:3343216:3343755 [2] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so libnccl-net.so. Using internal tuner plugin.
sophia-gpu-02:3343216:3343755 [2] NCCL INFO ncclCommInitRank comm 0x561b08925010 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 87000 commId 0xca63a03d39dbc1c9 - Init COMPLETE
sophia-gpu-02:3343216:3343755 [2] NCCL INFO Init timings: rank 2 nranks 4 total 0.50 (kernels 0.14, bootstrap 0.15, allgathers 0.00, topo 0.14, graphs 0.00, connections 0.05, rest 0.02)
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 00/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 02/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 03/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 04/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 05/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 06/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 07/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 08/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 09/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 10/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 11/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 12/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 13/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 14/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 15/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 16/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 17/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 18/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 19/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 20/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 21/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 22/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Channel 23/0 : 2[2] -> 0[0] via P2P/CUMEM/read
sophia-gpu-02:3343216:3343816 [2] NCCL INFO Connected all rings
PyTorch version: 2.5.0+cu124
CUDA available: True
Number of GPUs: 4
Lattice size: 64x64
Number of subsets: 8
Check Jacobian: False
Identity initialization: True
Continue training: False
Trying to use torch.compile for optimized computation...
Successfully initialized torch.compile
>>> Training from scratch
Loaded data shape: torch.Size([4096, 2, 64, 64])
Training data shape: torch.Size([3276, 2, 64, 64])
Testing data shape: torch.Size([820, 2, 64, 64])

>>> Training the model at beta = 2.0
sophia-gpu-02:3343217:3343217 [3] NCCL INFO cudaDriverVersion 12040
sophia-gpu-02:3343217:3343217 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker0
sophia-gpu-02:3343217:3343217 [3] NCCL INFO Bootstrap : Using enp226s0:10.230.2.190<0>
sophia-gpu-02:3343217:3343217 [3] NCCL INFO NCCL version 2.22.3+cuda12.4
sophia-gpu-02:3343217:3343756 [3] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal network plugin.
sophia-gpu-02:3343217:3343756 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker0
sophia-gpu-02:3343217:3343756 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/RoCE [6]mlx5_6:1/IB [7]mlx5_7:1/IB [8]mlx5_8:1/IB [9]mlx5_9:1/IB [10]mlx5_10:1/IB [11]mlx5_11:1/RoCE [RO]; OOB enp226s0:10.230.2.190<0>
sophia-gpu-02:3343217:3343756 [3] NCCL INFO Using network IB
sophia-gpu-02:3343217:3343756 [3] NCCL INFO ncclCommInitRank comm 0x55a5ec9acf40 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 90000 commId 0xca63a03d39dbc1c9 - Init START
sophia-gpu-02:3343217:3343756 [3] NCCL INFO Setting affinity for GPU 3 to ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000,00000000
sophia-gpu-02:3343217:3343756 [3] NCCL INFO NVLS multicast support is not available on dev 3
sophia-gpu-02:3343217:3343756 [3] NCCL INFO comm 0x55a5ec9acf40 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
sophia-gpu-02:3343217:3343756 [3] NCCL INFO Trees [0] 2/-1/-1->3->1 [1] 2/-1/-1->3->1 [2] 2/-1/-1->3->1 [3] 2/-1/-1->3->1 [4] 2/-1/-1->3->1 [5] 2/-1/-1->3->1 [6] 2/-1/-1->3->1 [7] 2/-1/-1->3->1 [8] 2/-1/-1->3->1 [9] 2/-1/-1->3->1 [10] 2/-1/-1->3->1 [11] 2/-1/-1->3->1 [12] 2/-1/-1->3->1 [13] 2/-1/-1->3->1 [14] 2/-1/-1->3->1 [15] 2/-1/-1->3->1 [16] 2/-1/-1->3->1 [17] 2/-1/-1->3->1 [18] 2/-1/-1->3->1 [19] 2/-1/-1->3->1 [20] 2/-1/-1->3->1 [21] 2/-1/-1->3->1 [22] 2/-1/-1->3->1 [23] 2/-1/-1->3->1
sophia-gpu-02:3343217:3343756 [3] NCCL INFO P2P Chunksize set to 524288
sophia-gpu-02:3343217:3343756 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
sophia-gpu-02:3343217:3343756 [3] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
sophia-gpu-02:3343217:3343756 [3] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so libnccl-net.so. Using internal tuner plugin.
sophia-gpu-02:3343217:3343756 [3] NCCL INFO ncclCommInitRank comm 0x55a5ec9acf40 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 90000 commId 0xca63a03d39dbc1c9 - Init COMPLETE
sophia-gpu-02:3343217:3343756 [3] NCCL INFO Init timings: rank 3 nranks 4 total 0.50 (kernels 0.13, bootstrap 0.15, allgathers 0.00, topo 0.14, graphs 0.00, connections 0.05, rest 0.02)
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/CUMEM/read
sophia-gpu-02:3343217:3343815 [3] NCCL INFO Connected all rings
Trying to use torch.compile for optimized computation...
Successfully initialized torch.compile
sophia-gpu-02:3343214:3343214 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker0
sophia-gpu-02:3343214:3343214 [0] NCCL INFO Bootstrap : Using enp226s0:10.230.2.190<0>
sophia-gpu-02:3343214:3343214 [0] NCCL INFO cudaDriverVersion 12040
sophia-gpu-02:3343214:3343214 [0] NCCL INFO NCCL version 2.22.3+cuda12.4
sophia-gpu-02:3343214:3343753 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal network plugin.
sophia-gpu-02:3343214:3343753 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker0
sophia-gpu-02:3343214:3343753 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/RoCE [6]mlx5_6:1/IB [7]mlx5_7:1/IB [8]mlx5_8:1/IB [9]mlx5_9:1/IB [10]mlx5_10:1/IB [11]mlx5_11:1/RoCE [RO]; OOB enp226s0:10.230.2.190<0>
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Using network IB
sophia-gpu-02:3343214:3343753 [0] NCCL INFO ncclCommInitRank comm 0x55e0fc033ce0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 47000 commId 0xca63a03d39dbc1c9 - Init START
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,00000000,00000000,ffff0000
sophia-gpu-02:3343214:3343753 [0] NCCL INFO NVLS multicast support is not available on dev 0
sophia-gpu-02:3343214:3343753 [0] NCCL INFO comm 0x55e0fc033ce0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 00/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 01/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 02/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 03/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 04/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 05/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 06/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 07/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 08/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 09/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 10/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 11/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 12/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 13/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 14/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 15/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 16/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 17/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 18/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 19/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 20/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 21/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 22/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Channel 23/24 :    0   1   3   2
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
sophia-gpu-02:3343214:3343753 [0] NCCL INFO P2P Chunksize set to 524288
sophia-gpu-02:3343214:3343753 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
sophia-gpu-02:3343214:3343753 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
sophia-gpu-02:3343214:3343753 [0] NCCL INFO CC Off, Multi-GPU CC Off, workFifoBytes 1048576
sophia-gpu-02:3343214:3343753 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so libnccl-net.so. Using internal tuner plugin.
sophia-gpu-02:3343214:3343753 [0] NCCL INFO ncclCommInitRank comm 0x55e0fc033ce0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 47000 commId 0xca63a03d39dbc1c9 - Init COMPLETE
sophia-gpu-02:3343214:3343753 [0] NCCL INFO Init timings: rank 0 nranks 4 total 0.51 (kernels 0.13, bootstrap 0.17, allgathers 0.00, topo 0.14, graphs 0.00, connections 0.05, rest 0.02)
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM/read
sophia-gpu-02:3343214:3343814 [0] NCCL INFO Connected all rings
sophia-gpu-02:3343215:3343215 [1] NCCL INFO cudaDriverVersion 12040
sophia-gpu-02:3343215:3343215 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker0
sophia-gpu-02:3343215:3343215 [1] NCCL INFO Bootstrap : Using enp226s0:10.230.2.190<0>
sophia-gpu-02:3343215:3343215 [1] NCCL INFO NCCL version 2.22.3+cuda12.4
sophia-gpu-02:3343215:3343754 [1] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal network plugin.
sophia-gpu-02:3343215:3343754 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker0
sophia-gpu-02:3343215:3343754 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/RoCE [6]mlx5_6:1/IB [7]mlx5_7:1/IB [8]mlx5_8:1/IB [9]mlx5_9:1/IB [10]mlx5_10:1/IB [11]mlx5_11:1/RoCE [RO]; OOB enp226s0:10.230.2.190<0>
sophia-gpu-02:3343215:3343754 [1] NCCL INFO Using network IB
sophia-gpu-02:3343215:3343754 [1] NCCL INFO ncclCommInitRank comm 0x55b492aa5c10 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 4e000 commId 0xca63a03d39dbc1c9 - Init START
sophia-gpu-02:3343215:3343754 [1] NCCL INFO Setting affinity for GPU 1 to ffff0000,00000000,00000000,00000000,ffff0000
sophia-gpu-02:3343215:3343754 [1] NCCL INFO NVLS multicast support is not available on dev 1
sophia-gpu-02:3343215:3343754 [1] NCCL INFO comm 0x55b492aa5c10 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
sophia-gpu-02:3343215:3343754 [1] NCCL INFO Trees [0] 3/-1/-1->1->0 [1] 3/-1/-1->1->0 [2] 3/-1/-1->1->0 [3] 3/-1/-1->1->0 [4] 3/-1/-1->1->0 [5] 3/-1/-1->1->0 [6] 3/-1/-1->1->0 [7] 3/-1/-1->1->0 [8] 3/-1/-1->1->0 [9] 3/-1/-1->1->0 [10] 3/-1/-1->1->0 [11] 3/-1/-1->1->0 [12] 3/-1/-1->1->0 [13] 3/-1/-1->1->0 [14] 3/-1/-1->1->0 [15] 3/-1/-1->1->0 [16] 3/-1/-1->1->0 [17] 3/-1/-1->1->0 [18] 3/-1/-1->1->0 [19] 3/-1/-1->1->0 [20] 3/-1/-1->1->0 [21] 3/-1/-1->1->0 [22] 3/-1/-1->1->0 [23] 3/-1/-1->1->0
sophia-gpu-02:3343215:3343754 [1] NCCL INFO P2P Chunksize set to 524288
sophia-gpu-02:3343215:3343754 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
sophia-gpu-02:3343215:3343754 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
sophia-gpu-02:3343215:3343754 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so libnccl-net.so. Using internal tuner plugin.
sophia-gpu-02:3343215:3343754 [1] NCCL INFO ncclCommInitRank comm 0x55b492aa5c10 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 4e000 commId 0xca63a03d39dbc1c9 - Init COMPLETE
sophia-gpu-02:3343215:3343754 [1] NCCL INFO Init timings: rank 1 nranks 4 total 0.50 (kernels 0.14, bootstrap 0.15, allgathers 0.00, topo 0.14, graphs 0.00, connections 0.04, rest 0.02)
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 00/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 01/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 02/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 03/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 04/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 05/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 06/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 07/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 08/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 09/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 10/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 11/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 12/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 13/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 14/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 15/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 16/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 17/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 18/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 19/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 20/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 21/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 22/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Channel 23/0 : 1[1] -> 3[3] via P2P/CUMEM/read
sophia-gpu-02:3343215:3343813 [1] NCCL INFO Connected all rings
[rank0]:[W628 02:42:05.042454459 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
sophia-gpu-02:3343214:3361721 [0] NCCL INFO misc/socket.cc:47 -> 3
sophia-gpu-02:3343214:3361721 [0] NCCL INFO misc/socket.cc:550 -> 3
sophia-gpu-02:3343214:3361721 [0] NCCL INFO misc/socket.cc:573 -> 3
sophia-gpu-02:3343214:3361721 [0] NCCL INFO misc/socket.cc:621 -> 3
sophia-gpu-02:3343214:3361721 [0] NCCL INFO misc/socket.cc:47 -> 3
sophia-gpu-02:3343214:3361721 [0] NCCL INFO misc/socket.cc:58 -> 3
sophia-gpu-02:3343214:3361721 [0] NCCL INFO misc/socket.cc:775 -> 3
sophia-gpu-02:3343214:3343810 [0] NCCL INFO misc/socket.cc:47 -> 3
sophia-gpu-02:3343214:3343810 [0] NCCL INFO misc/socket.cc:752 -> 3
sophia-gpu-02:3343214:3343810 [0] NCCL INFO misc/socket.cc:428 -> 3
sophia-gpu-02:3343214:3343810 [0] NCCL INFO misc/socket.cc:564 -> 3
sophia-gpu-02:3343214:3343810 [0] NCCL INFO misc/socket.cc:668 -> 3

sophia-gpu-02:3343214:3343810 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
sophia-gpu-02:3343214:3343810 [0] NCCL INFO misc/socket.cc:826 -> 3

sophia-gpu-02:3343214:3343810 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0

sophia-gpu-02:3343214:3343810 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 0, retcode 3
sophia-gpu-02:3343214:3361721 [0] NCCL INFO comm 0x55e0fc033ce0 rank 0 nranks 4 cudaDev 0 busId 47000 - Abort COMPLETE
sophia-gpu-02:3343215:3361722 [1] NCCL INFO misc/socket.cc:47 -> 3
sophia-gpu-02:3343215:3361722 [1] NCCL INFO misc/socket.cc:550 -> 3
sophia-gpu-02:3343215:3361722 [1] NCCL INFO misc/socket.cc:573 -> 3
sophia-gpu-02:3343215:3361722 [1] NCCL INFO misc/socket.cc:621 -> 3
sophia-gpu-02:3343215:3343805 [1] NCCL INFO misc/socket.cc:47 -> 3
sophia-gpu-02:3343215:3343805 [1] NCCL INFO misc/socket.cc:752 -> 3
sophia-gpu-02:3343215:3343805 [1] NCCL INFO misc/socket.cc:428 -> 3
sophia-gpu-02:3343215:3343805 [1] NCCL INFO misc/socket.cc:564 -> 3
sophia-gpu-02:3343215:3343805 [1] NCCL INFO misc/socket.cc:668 -> 3

sophia-gpu-02:3343215:3343805 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
sophia-gpu-02:3343215:3361722 [1] NCCL INFO misc/socket.cc:47 -> 3
sophia-gpu-02:3343215:3361722 [1] NCCL INFO misc/socket.cc:58 -> 3
sophia-gpu-02:3343215:3361722 [1] NCCL INFO misc/socket.cc:775 -> 3
sophia-gpu-02:3343215:3343805 [1] NCCL INFO misc/socket.cc:826 -> 3

sophia-gpu-02:3343215:3343805 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0

sophia-gpu-02:3343215:3343805 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 1, retcode 3
sophia-gpu-02:3343215:3361722 [1] NCCL INFO comm 0x55b492aa5c10 rank 1 nranks 4 cudaDev 1 busId 4e000 - Abort COMPLETE
sophia-gpu-02:3343217:3361720 [3] NCCL INFO misc/socket.cc:47 -> 3
sophia-gpu-02:3343217:3361720 [3] NCCL INFO misc/socket.cc:550 -> 3
sophia-gpu-02:3343217:3361720 [3] NCCL INFO misc/socket.cc:573 -> 3
sophia-gpu-02:3343217:3361720 [3] NCCL INFO misc/socket.cc:621 -> 3
sophia-gpu-02:3343217:3361720 [3] NCCL INFO misc/socket.cc:47 -> 3
sophia-gpu-02:3343217:3361720 [3] NCCL INFO misc/socket.cc:58 -> 3
sophia-gpu-02:3343217:3361720 [3] NCCL INFO misc/socket.cc:775 -> 3
sophia-gpu-02:3343217:3343806 [3] NCCL INFO misc/socket.cc:47 -> 3
sophia-gpu-02:3343217:3343806 [3] NCCL INFO misc/socket.cc:752 -> 3
sophia-gpu-02:3343217:3343806 [3] NCCL INFO misc/socket.cc:428 -> 3
sophia-gpu-02:3343217:3343806 [3] NCCL INFO misc/socket.cc:564 -> 3
sophia-gpu-02:3343217:3343806 [3] NCCL INFO misc/socket.cc:668 -> 3

sophia-gpu-02:3343217:3343806 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
sophia-gpu-02:3343217:3343806 [3] NCCL INFO misc/socket.cc:826 -> 3

sophia-gpu-02:3343217:3343806 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0

sophia-gpu-02:3343217:3343806 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 3, retcode 3
sophia-gpu-02:3343217:3361720 [3] NCCL INFO comm 0x55a5ec9acf40 rank 3 nranks 4 cudaDev 3 busId 90000 - Abort COMPLETE
sophia-gpu-02:3343216:3361723 [2] NCCL INFO misc/socket.cc:47 -> 3
sophia-gpu-02:3343216:3361723 [2] NCCL INFO misc/socket.cc:550 -> 3
sophia-gpu-02:3343216:3361723 [2] NCCL INFO misc/socket.cc:573 -> 3
sophia-gpu-02:3343216:3361723 [2] NCCL INFO misc/socket.cc:621 -> 3
sophia-gpu-02:3343216:3361723 [2] NCCL INFO misc/socket.cc:47 -> 3
sophia-gpu-02:3343216:3361723 [2] NCCL INFO misc/socket.cc:58 -> 3
sophia-gpu-02:3343216:3361723 [2] NCCL INFO misc/socket.cc:775 -> 3
sophia-gpu-02:3343216:3343809 [2] NCCL INFO misc/socket.cc:47 -> 3
sophia-gpu-02:3343216:3343809 [2] NCCL INFO misc/socket.cc:752 -> 3
sophia-gpu-02:3343216:3343809 [2] NCCL INFO misc/socket.cc:428 -> 3
sophia-gpu-02:3343216:3343809 [2] NCCL INFO misc/socket.cc:564 -> 3
sophia-gpu-02:3343216:3343809 [2] NCCL INFO misc/socket.cc:668 -> 3

sophia-gpu-02:3343216:3343809 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
sophia-gpu-02:3343216:3343809 [2] NCCL INFO misc/socket.cc:826 -> 3

sophia-gpu-02:3343216:3343809 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0

sophia-gpu-02:3343216:3343809 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 2, retcode 3
sophia-gpu-02:3343216:3361723 [2] NCCL INFO comm 0x561b08925010 rank 2 nranks 4 cudaDev 2 busId 87000 - Abort COMPLETE
Traceback (most recent call last):
  File "/lus/eagle/projects/fthmc/run/hmc_ft/2d_u1_cluster/train_ddp.py", line 288, in <module>
    main() 
  File "/lus/eagle/projects/fthmc/run/hmc_ft/2d_u1_cluster/train_ddp.py", line 280, in main
    mp.spawn(train_ddp, args=(world_size, args), nprocs=world_size, join=True)
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/multiprocessing/spawn.py", line 328, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/multiprocessing/spawn.py", line 284, in start_processes
    while not context.join():
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/multiprocessing/spawn.py", line 203, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/multiprocessing/spawn.py", line 90, in _wrap
    fn(i, *args)
  File "/lus/eagle/projects/fthmc/run/hmc_ft/2d_u1_cluster/train_ddp.py", line 148, in train_ddp
    train_ddp_model(nn_ft, train_loader, test_loader, train_beta, args, rank, world_size)
  File "/lus/eagle/projects/fthmc/run/hmc_ft/2d_u1_cluster/train_ddp.py", line 202, in train_ddp_model
    loss = nn_ft.evaluate_step(batch)
  File "/lus/eagle/projects/fthmc/run/hmc_ft/2d_u1_cluster/field_trans.py", line 444, in evaluate_step
    loss = self.loss_fn(theta_ori)
  File "/lus/eagle/projects/fthmc/run/hmc_ft/2d_u1_cluster/field_trans.py", line 386, in loss_fn
    force_ori = self.compute_force(theta_new, beta=1) #todo
  File "/lus/eagle/projects/fthmc/run/hmc_ft/2d_u1_cluster/field_trans.py", line 375, in compute_force
    grad = torch.autograd.grad(total_action[i], theta, create_graph=True)[0]
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/autograd/__init__.py", line 496, in grad
    result = _engine_run_backward(
  File "/lus/eagle/projects/fthmc/software/ml/lib64/python3.9/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

End time: 2025-06-28 02:42:09
Total time: 0h 4m 42s
